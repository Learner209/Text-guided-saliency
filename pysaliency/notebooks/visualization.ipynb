{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the sjtuvis dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from PIL import Image\n",
    "from textwrap import wrap\n",
    "\n",
    "from pysaliency.external_datasets.sjtuvis import TextDescriptor\n",
    "\n",
    "data_location = \"../../datasets/test\"\n",
    "text_descriptor = TextDescriptor('../../datasets/test/original_sjtuvis_dataset/text.xlsx')\n",
    "\n",
    "def load_images_with_versions(folder, versions):\n",
    "    images = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        basename, ext = os.path.splitext(filename)\n",
    "        key_basename = basename\n",
    "        for version in versions:\n",
    "            key_basename = key_basename.replace(f'_{version}', '')\n",
    "        if key_basename not in images:\n",
    "            images[key_basename] = {}\n",
    "\n",
    "        for version in versions:\n",
    "            if basename.endswith(f'_{version}') or (version == 'overall' and '_' not in basename):\n",
    "                img = Image.open(os.path.join(folder, filename))\n",
    "                if img is not None:\n",
    "                    images[key_basename][version] = img\n",
    "    return images\n",
    "\n",
    "def create_image_grid_with_captions(ori_images, map_images, get_caption, rows=8, cols=10):\n",
    "    fig = plt.figure(figsize=(40, 30))  # Adjusted figure size\n",
    "    gs = gridspec.GridSpec(rows, cols, hspace=0.4, wspace=0.0)  # Reduced hspace\n",
    "\n",
    "    for i, (basename, versions) in enumerate(ori_images.items()):\n",
    "        row = i // (cols // 5) \n",
    "        col = (i % (cols // 5)) * 5\n",
    "\n",
    "        ax = plt.subplot(gs[row, col:col+1])\n",
    "        ori_image = ori_images[basename]['0'].resize((1280, 960))\n",
    "        ax.imshow(ori_image)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        for j, (version, img) in enumerate(versions.items()):\n",
    "            ax = plt.subplot(gs[row, col + j + 1:col + j + 2])\n",
    "            map_image = map_images[basename][version].resize((1280, 960))\n",
    "            ax.imshow(map_image)\n",
    "            text_description = text_descriptor.get_description(f'{basename}_{version}.png')\n",
    "            title = ax.set_title(\"\\n\".join(wrap(text_description, 35)), fontsize=15)\n",
    "            title.set_y(1.05)\n",
    "            title.set_multialignment('center')\n",
    "            title.set_linespacing(1.5)\n",
    "            ax.axis('off')\n",
    "\n",
    "        if i >= rows * cols // 5 - 1:\n",
    "            break\n",
    "\n",
    "    plt.subplots_adjust(top=0.99, bottom=0.01, left=0.01, right=0.99, hspace=0.01, wspace=0.01)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Dummy function for generating captions\n",
    "def get_caption(version, basename):\n",
    "    return f\"Caption for version {version} of {basename}\"\n",
    "\n",
    "# Load your images\n",
    "# Replace 'path_to_images' with the actual path to your images\n",
    "\n",
    "ori_images = load_images_with_versions('../../datasets/test/original_sjtuvis_dataset/image', versions=['overall', '0', '1', '2', '3'])\n",
    "map_images = load_images_with_versions('../../datasets/test/original_sjtuvis_dataset/map', versions=['overall', '0', '1', '2', '3'])\n",
    "\n",
    "ori_images = {k: v for k, v in ori_images.items() if len(v.keys()) == 4}\n",
    "map_images = {k: v for k, v in map_images.items() if len(v.keys()) == 4}\n",
    "# print(ori_images)\n",
    "# print(map_images)\n",
    "\n",
    "create_image_grid_with_captions(ori_images,map_images, get_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the model from stimuli.hdf5 and fixation.hdf5 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pysaliency\n",
    "from pysaliency.external_datasets.sjtuvis import TextDescriptor\n",
    "import matplotlib.pyplot as plt\n",
    "import pysaliency\n",
    "\n",
    "mit_stimuli, mit_fixations = pysaliency.external_datasets.get_mit1003_onesize(location=\"../../datasets/\")\n",
    "mit_stimuli, mit_fixations = pysaliency.external_datasets.get_DUT_OMRON(location=\"../../datasets/\")\n",
    "# mit_stimuli, mit_fixations = pysaliency.external_datasets.get_FIGRIM(location=\"../../datasets/\")\n",
    "# mit_stimuli, mit_fixations = pysaliency.external_datasets.get_NUSEF_public(location=\"../../datasets/\")\n",
    "# mit_stimuli, mit_fixations = pysaliency.external_datasets.get_OSIE(location=\"../../datasets/\")\n",
    "# stimuli_train, stimuli_val, stimuli_test, fixations_train, fixations_val = pysaliency.external_datasets.get_SALICON(location=\"../../datasets/\")\n",
    "# mit_stimuli, mit_fixations = stimuli_train, fixations_train\n",
    "mit_stimuli, mit_fixations = pysaliency.external_datasets.get_PASCAL_S(location=\"../../datasets/\")\n",
    "mit_stimuli, mit_fixations = pysaliency.external_datasets.get_toronto(location=\"../../datasets/\")\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_image_grid(mit_stimuli, mit_fixations, rows=18, cols=6):\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 85))\n",
    "\n",
    "    i = 0\n",
    "    cut_off = rows * cols // 2\n",
    "    for i, stimulus in enumerate(mit_stimuli[:cut_off]):\n",
    "        fixation = mit_fixations[mit_fixations.n == i] \n",
    "             \n",
    "        # img = Image.fromarray(stimulus.stimulus_data)\n",
    "        # img = img.resize((640, 480))\n",
    "        img = stimulus.stimulus_data\n",
    "        row, col = divmod(i, cols // 2)\n",
    "\n",
    "        ax1 = fig.add_subplot(rows, cols, row * cols + col * 2 + 1)\n",
    "        ax2 = fig.add_subplot(rows, cols, row * cols + col * 2 + 2)\n",
    "\n",
    "        ax1.imshow(img)\n",
    "        ax2.imshow(img)\n",
    "        ax2.scatter(fixation.x, fixation.y, c='red', s=10)\n",
    "\n",
    "        ax1.axis('off')\n",
    "        ax2.axis('off')\n",
    "\n",
    "        i += 1\n",
    "        if i >= rows * cols // 2:\n",
    "            break\n",
    "\n",
    "    # plt.subplots_adjust(wspace=0, hspace=0) \n",
    "    plt.subplots_adjust(bottom=0.3, top=0.7, hspace=0, wspace = 0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "create_image_grid(mit_stimuli, mit_fixations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cat Train set visualization !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            img = Image.open(os.path.join(folder, filename))\n",
    "            if img is not None:\n",
    "                images[filename.split('.')[0]] = img\n",
    "    return images\n",
    "\n",
    "def create_image_grid(original_images, saliency_maps, rows=18, cols=6):\n",
    "    assert len(original_images) == len(saliency_maps)\n",
    "    \n",
    "    # Calculate the size of each subplot to fit the grid perfectly\n",
    "    subplot_width = 1.0 / cols\n",
    "    subplot_height = 1.0 / rows\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 85))\n",
    "\n",
    "    i = 0\n",
    "    for key, original_img in original_images.items():\n",
    "        saliency_img = saliency_maps.get(key + '_SaliencyMap')\n",
    "        if not saliency_img:\n",
    "            continue\n",
    "\n",
    "        row, col = divmod(i, cols // 2)\n",
    "\n",
    "        ax1 = fig.add_subplot(rows, cols, row * cols + col * 2 + 1)\n",
    "        ax2 = fig.add_subplot(rows, cols, row * cols + col * 2 + 2)\n",
    "\n",
    "        ax1.imshow(original_img)\n",
    "        ax2.imshow(saliency_img)\n",
    "\n",
    "        ax1.axis('off')\n",
    "        ax2.axis('off')\n",
    "\n",
    "        i += 1\n",
    "        if i >= rows * cols // 2:\n",
    "            break\n",
    "\n",
    "    # plt.subplots_adjust(wspace=0, hspace=0) \n",
    "    plt.subplots_adjust(bottom=0.0, top=0.0, hspace=0, wspace = 0)\n",
    "    plt.show()\n",
    "\n",
    "original_images = load_images_from_folder('../../datasets/cat2000/testSet/Stimuli/Art')\n",
    "saliency_maps = load_images_from_folder('../../datasets/cat2000/testSet/Stimuli/Art/Output/')\n",
    "\n",
    "create_image_grid(original_images, saliency_maps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atttain the spatial information, brightness, colorfulness and the contrast of the dataeset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage import color, filters\n",
    "\n",
    "def is_grayscale(img):\n",
    "    \"\"\"Check if the image is grayscale.\"\"\"\n",
    "    if len(img.shape) < 3: return True\n",
    "    if img.shape[2] == 1: return True\n",
    "    return np.all(img[:, :, 0] == img[:, :, 1]) and np.all(img[:, :, 1] == img[:, :, 2])\n",
    "\n",
    "def contrast(img):\n",
    "    \"\"\"Contrast for grayscale or color images.\"\"\"\n",
    "    if is_grayscale(img):\n",
    "        std_dev = np.std(img)\n",
    "    else:\n",
    "        gray_img = color.rgb2gray(img)\n",
    "        std_dev = np.std(gray_img)\n",
    "    return std_dev\n",
    "\n",
    "def colorfulness(img):\n",
    "    \"\"\"Colorfulness metric, return 0 for grayscale images.\"\"\"\n",
    "    if is_grayscale(img):\n",
    "        return 0\n",
    "    R, G, B = img[:, :, 0], img[:, :, 1], img[:, :, 2]\n",
    "    rg = R - G\n",
    "    yb = 0.5 * (R + G) - B\n",
    "    std_rg, std_yb = np.std(rg), np.std(yb)\n",
    "    mean_rg, mean_yb = np.mean(rg), np.mean(yb)\n",
    "    return np.sqrt(std_rg**2 + std_yb**2 + 0.3 * (mean_rg**2 + mean_yb**2))\n",
    "\n",
    "def spatial_info(img):\n",
    "    \"\"\"Spatial information for grayscale or color images.\"\"\"\n",
    "    if is_grayscale(img):\n",
    "        Gmag = filters.sobel(img)\n",
    "    else:\n",
    "        gray_img = color.rgb2gray(img)\n",
    "        Gmag = filters.sobel(gray_img)\n",
    "    Gdir = np.arctan2(np.gradient(Gmag)[0], np.gradient(Gmag)[1])\n",
    "    return 0.5 * (np.std(Gmag) + np.std(Gdir))\n",
    "\n",
    "def brightness(img):\n",
    "    \"\"\"Brightness for grayscale or color images.\"\"\"\n",
    "    if is_grayscale(img):\n",
    "        return img\n",
    "    R, G, B = img[:, :, 0], img[:, :, 1], img[:, :, 2]\n",
    "    return 0.299 * R + 0.587 * G + 0.114 * B\n",
    "\n",
    "\n",
    "def process_images(image_folder, dataset_name):\n",
    "    # print(\"dataset name is\", dataset_name)\n",
    "    brightness_vals, contrast_vals, colorfulness_vals, spatial_vals = [], [], [], []\n",
    "    for filename in os.listdir(image_folder):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            print(os.path.join(image_folder, filename))\n",
    "            img = Image.open(os.path.join(image_folder, filename))\n",
    "            img_np = np.array(img)\n",
    "            if img_np.shape[-1] == 4 and len(img_np.shape) == 3:\n",
    "                img_np = img_np[...,:3]\n",
    "            brightness_vals.append(np.mean(brightness(img_np)))\n",
    "            contrast_vals.append(contrast(img_np))\n",
    "            colorfulness_vals.append(colorfulness(img_np))\n",
    "            spatial_vals.append(spatial_info(img_np))\n",
    "\n",
    "    return {\n",
    "    'name': dataset_name,\n",
    "    'brightness': brightness_vals,\n",
    "    'contrast': contrast_vals,\n",
    "    'colorfulness': colorfulness_vals,\n",
    "    'spatial': spatial_vals\n",
    "    }\n",
    "\n",
    "def plot_distributions(datasets, metric):\n",
    "    for dataset in datasets:\n",
    "        sns.kdeplot(dataset[metric], label=dataset['name'])\n",
    "    plt.title(f\"{metric.capitalize()} Distribution\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Modified plot_distributions function\n",
    "def plot_distributions(datasets):\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))  # 1 row, 4 columns of subplots\n",
    "    metrics = ['brightness', 'contrast', 'colorfulness', 'spatial']\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i]\n",
    "        for dataset in datasets:\n",
    "            sns.kdeplot(dataset[metric], label=dataset['name'], ax=ax)\n",
    "        ax.set_title(f\"{metric.capitalize()} Distribution\")\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(metric.capitalize())\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Path to your image dataset\n",
    "image_folder = '/home/liilu/Desktop/COURSE/ImageProcessing/Assignment2/BASELINE/datasets/DUT-OMRON/Stimuli'\n",
    "\n",
    "# List of datasets\n",
    "datasets_info = [\n",
    "    {\n",
    "        'path': '../../datasets/cat2000/testSet/Stimuli/Art',\n",
    "        'name': 'CAT2000'\n",
    "    },\n",
    "    {\n",
    "        'path': '../../datasets/DUT-OMRON/Stimuli',\n",
    "        'name': 'DUT-OMRON'\n",
    "    },\n",
    "    {\n",
    "        'path': '../../datasets/FIGRIM/Stimuli/Fillers/highway',\n",
    "        'name': 'FIGRIM'\n",
    "    },\n",
    "    {\n",
    "        'path': '../../datasets/MIT300/stimuli',\n",
    "        'name': 'MIT300'\n",
    "    },\n",
    "    {\n",
    "        'path': '../../datasets/NUSEF_public/Stimuli',\n",
    "        'name': 'NUSEF-public'\n",
    "    },\n",
    "    {\n",
    "        'path': '../../datasets/OSIE/Stimuli',\n",
    "        'name': 'OSIE'\n",
    "    },\n",
    "    {\n",
    "        'path': '../../datasets/PASCAL-S/Stimuli',\n",
    "        'name': 'PASCAL-S'\n",
    "    },    \n",
    "    {\n",
    "        'path': '../../datasets/SALICON/stimuli/train',\n",
    "        'name': 'SALICON'\n",
    "    },    \n",
    "    {\n",
    "        'path': '../../datasets/sjtuvis/stimuli',\n",
    "        'name': 'sjtuvis'\n",
    "    },\n",
    "    {\n",
    "        'path': '../../datasets/toronto/stimuli',\n",
    "        'name': 'toronto'\n",
    "    },\n",
    "]\n",
    "\n",
    "processed_datasets = [process_images(info['path'], info['name']) for info in datasets_info]\n",
    "\n",
    "# Plotting distributions for each metric\n",
    "for metric in ['brightness', 'contrast', 'colorfulness', 'spatial']:\n",
    "    plot_distributions(processed_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pysaliency\n",
    "==========\n",
    "\n",
    "Saliency Map Models\n",
    "----------------------\n",
    "\n",
    "`pysaliency` comes with a variety of features to evaluate saliency map models. This notebooks demonstrates these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the MIT1003 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pysaliency\n",
    "from pysaliency.external_datasets.sjtuvis import TextDescriptor\n",
    "\n",
    "data_location = \"../../datasets/test\"\n",
    "text_descriptor = TextDescriptor('../../datasets/test/original_sjtuvis_dataset/text.xlsx')\n",
    "print(text_descriptor.get_description('000000020777_2.png')) \n",
    "mit_stimuli, mit_fixations = pysaliency.external_datasets.get_sjtu_vis(\"../../datasets/test/original_sjtuvis_dataset\", location=data_location, text_descriptor=text_descriptor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As some evaluation methods can take quite a long time to run, we prepare a smaller dataset consisting of only the first 10 stimuli:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the saliency model *AIM* by Bruce and Tsotos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_location = \"../../datasets\"\n",
    "mit_stimuli, mit_fixations = pysaliency.external_datasets.get_mit1003(location=data_location)\n",
    "index = 10\n",
    "plt.imshow(mit_stimuli.stimuli[index])\n",
    "f = mit_fixations[mit_fixations.n == index]\n",
    "plt.scatter(f.x, f.y, color='r')\n",
    "_ = plt.axis('off')\n",
    "cutoff = 20\n",
    "\n",
    "model = pysaliency.external_models.A>IM(location=\"../../models/\", \tcache_location=os.path.join('../../models/model_caches', 'AIM'), caching=True)\n",
    "text_descriptor = TextDescriptor('../../datasets/test/original_sjtuvis_dataset/text.xlsx')\n",
    "mit_stimuli, mit_fixations = pysaliency.external_datasets.get_sjtu_vis(original_dataset_path=\"../../datasets/test/original_sjtuvis_dataset\", location='../../datasets', text_descriptor=text_descriptor)\n",
    "cutoff = 10\n",
    "short_stimuli = pysaliency.FileStimuli(filenames=mit_stimuli.filenames[:cutoff])\n",
    "short_fixations = mit_fixations[mit_fixations.n < cutoff]\n",
    "auc_uniform = model.AUC(short_stimuli, short_fixations, nonfixations='uniform', verbose=True)\n",
    "auc_shuffled = model.AUC(short_stimuli, short_fixations, nonfixations='shuffled', verbose=True)\n",
    "auc_identical_nonfixations = model.AUC(short_stimuli, short_fixations, nonfixations=short_fixations, verbose=True)\n",
    "kl_uniform = model.fixation_based_KL_divergence(short_stimuli, short_fixations, nonfixations='uniform')\n",
    "kl_shuffled = model.fixation_based_KL_divergence(short_stimuli, short_fixations, nonfixations='shuffled')\n",
    "kl_identical = model.fixation_based_KL_divergence(short_stimuli, short_fixations, nonfixations=short_fixations)\n",
    "nss = model.NSS(short_stimuli, short_fixations)\n",
    "gold_standard = pysaliency.FixationMap(short_stimuli, short_fixations, kernel_size=30)\n",
    "image_based_kl = model.image_based_kl_divergence(short_stimuli, gold_standard)\n",
    "cc = model.CC(short_stimuli, gold_standard)\n",
    "ssim = model.SIM(short_stimuli, gold_standard)\n",
    "print(\"AUC Uniform: \", auc_uniform, \"AUC Shuffled: \", auc_shuffled, \"AUC Identical: \", auc_identical_nonfixations, \"KL Uniform: \", kl_uniform, \"KL Shuffled: \", kl_shuffled, \"KL Identical: \", kl_identical, \"NSS: \", nss, \"Image Based KL: \", image_based_kl, \"CC: \", cc, \"SSIM: \", ssim)\n",
    "\n",
    "from pysaliency.external_datasets.sjtuvis import TextDescriptor\n",
    "text_descriptor = TextDescriptor('../../datasets/test/original_sjtuvis_dataset/text.xlsx')\n",
    "data_location = \"../../datasets/test\"\n",
    "original_dataset_path = \"../../datasets/test/original_sjtuvis_dataset\"\n",
    "mit_stimuli, mit_fixations = pysaliency.external_datasets.get_sjtu_vis(original_dataset_path=original_dataset_path, location=data_location, text_descriptor = text_descriptor)\n",
    "short_stimuli = pysaliency.FileStimuli(filenames=mit_stimuli.filenames[:cutoff])\n",
    "short_fixations = mit_fixations[mit_fixations.n < cutoff]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lavis text guided saliency model. Loaded from the lavis module. 😎 😎 😎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 09:31:47.082850: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-13 09:31:47.203233: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from lavis.models import load_model_and_preprocess\n",
    "from lavis.processors import load_processor\n",
    "from matplotlib import pyplot as plt\n",
    "from lavis.common.gradcam import getAttMap\n",
    "from lavis.models.blip_models.blip_image_text_matching import compute_gradcam\n",
    "import numpy as np\n",
    "from pysaliency.datasets import Stimulus, Fixations,  StimuliStimulus\n",
    "import pysaliency\n",
    "\n",
    "def handle_stimulus(stimulus):\n",
    "    \"\"\"\n",
    "    Make sure that a stimulus is a `Stimulus`-object\n",
    "    \"\"\"\n",
    "    if not isinstance(stimulus, Stimulus):\n",
    "        stimulus = Stimulus(stimulus)\n",
    "    return stimulus\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.misc import face\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.special import logsumexp\n",
    "import torch\n",
    "\n",
    "import deepgaze_pytorch    \n",
    "from typing import Optional, Tuple, List, Dict, Any, Union, Sequence\n",
    "import pysaliency\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "class DeepGazeSaliencyModel(pysaliency.SaliencyMapModel):\n",
    "    \n",
    "    def __init__(self, block_num=7, dst_w=720, version = 1):\n",
    "        super().__init__()\n",
    "        if version == 1:\n",
    "            self.model = deepgaze_pytorch.DeepGazeI(pretrained=True).to(DEVICE)\n",
    "            self.use_hist_scan_path = False\n",
    "        elif version == 2:\n",
    "            self.model = deepgaze_pytorch.DeepGazeIIE(pretrained=True).to(DEVICE)\n",
    "            self.use_hist_scan_path = False\n",
    "        elif version == 3:\n",
    "            self.model = deepgaze_pytorch.DeepGazeIII(pretrained=True).to(DEVICE)\n",
    "            self.use_hist_scan_path = True\n",
    "        self.block_num = block_num\n",
    "        self.dst_w = dst_w\n",
    "        \n",
    "    def _saliency_map(self, stimulus, text_description:Optional[str]=None):\n",
    "        image = stimulus\n",
    "        # print(\"The input image....\")\n",
    "        # print(image.shape, image.dtype, np.min(image), np.max(image))\n",
    "        fixation_history_x = np.array([1024//2, 300, 500, 200, 200, 700])\n",
    "        fixation_history_y = np.array([768//2, 300, 100, 300, 100, 500])\n",
    "\n",
    "        centerbias_template = np.load('../../pretrained_weights/deepgaze/centerbias_mit1003.npy')\n",
    "        centerbias = zoom(centerbias_template, (image.shape[0]/centerbias_template.shape[0], image.shape[1]/centerbias_template.shape[1]), order=0, mode='nearest')\n",
    "        centerbias -= logsumexp(centerbias)\n",
    "\n",
    "        image_tensor = torch.tensor([image.transpose(2, 0, 1)]).to(DEVICE)\n",
    "        centerbias_tensor = torch.tensor([centerbias]).to(DEVICE)\n",
    "\n",
    "        if self.use_hist_scan_path:\n",
    "            x_hist_tensor = torch.tensor([fixation_history_x[self.model.included_fixations]]).to(DEVICE)\n",
    "            y_hist_tensor = torch.tensor([fixation_history_y[self.model.included_fixations]]).to(DEVICE)\n",
    "            log_density_prediction = self.model(image_tensor, centerbias_tensor, x_hist_tensor, y_hist_tensor)\n",
    "        else:\n",
    "            log_density_prediction = self.model(image_tensor, centerbias_tensor)\n",
    "\n",
    "        if log_density_prediction.dim() == 2:\n",
    "            pass\n",
    "        elif log_density_prediction.dim() == 4:\n",
    "            log_density_prediction = log_density_prediction[0, 0]\n",
    "        elif log_density_prediction.dim() == 3:\n",
    "            print(log_density_prediction.shape)\n",
    "            assert log_density_prediction.shape[0] == 1\n",
    "            log_density_prediction = log_density_prediction[0]\n",
    "        assert log_density_prediction.dim() == 2, \"log_density_prediction should be a 3D tensor, but is {}\".format(log_density_prediction.shape)\n",
    "        return log_density_prediction.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "class LavisTextGuidedSaliencyModel(pysaliency.SaliencyMapModel):\n",
    "    \n",
    "    def __init__(self, debug_vis_flag = False, cache_location=None, caching=True, memory_cache_size=None):\n",
    "        super().__init__(cache_location, caching, memory_cache_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model, self.vis_processors, self.text_processors = load_model_and_preprocess(\"blip_image_text_matching\", \"large\", device=self.device, is_eval=True)\n",
    "        self.debug_vis_flag = debug_vis_flag\n",
    "        self.deepgaze_model = DeepGazeSaliencyModel(version=3)\n",
    "        \n",
    "    def saliency_map(self, stimulus):\n",
    "        \"\"\"\n",
    "        Get saliency map for given stimulus.\n",
    "\n",
    "        To overwrite this function, overwrite `_saliency_map` as otherwise\n",
    "        the caching mechanism is disabled.\n",
    "        \"\"\"\n",
    "        if isinstance(stimulus, StimuliStimulus):\n",
    "            filename = os.path.basename(stimulus.filename)\n",
    "        elif isinstance(stimulus, Stimulus):\n",
    "            filename = None\n",
    "        elif isinstance(stimulus, np.ndarray):\n",
    "            filename = None\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        stimulus = handle_stimulus(stimulus)\n",
    "            \n",
    "        try:\n",
    "            text_description=text_descriptor.get_description(filename)\n",
    "        except Exception as e:\n",
    "            text_description=None\n",
    "        # print(text_description, stimulus.filename)\n",
    "        if text_description is None:\n",
    "            return self.deepgaze_model.saliency_map(stimulus.stimulus_data)\n",
    "        else:\n",
    "            # print(text_description)\n",
    "            pass\n",
    "        if not self.caching:\n",
    "            return self._saliency_map(stimulus.stimulus_data, text_description=text_description)\n",
    "        stimulus_id = stimulus.stimulus_id\n",
    "        if not stimulus_id in self._cache:\n",
    "            temp = stimulus.stimulus_data\n",
    "            self._cache[stimulus_id] = self._saliency_map(temp, text_description=text_description)\n",
    "        return self._cache[stimulus_id]\n",
    "    \n",
    "    def _saliency_map(self, stimulus, text_description=None):\n",
    "        \n",
    "        # print(f\"****************      {text_description}      *********************\")\n",
    "        \n",
    "        return self.evaluate_saliency_map(stimulus[...,:3], \"large\", text_description)\n",
    "    \n",
    "    def evaluate_saliency_map(self, raw_image, model_path, caption, block_num=7, dst_w=720):\n",
    "        # print(raw_image.shape, raw_image.dtype, np.max(raw_image), np.min(raw_image))\n",
    "        raw_image = Image.fromarray(np.uint8(raw_image)).convert('RGB')\n",
    "        norm_img = np.float32(raw_image) / 255\n",
    "\n",
    "        # Preprocess image and text inputs\n",
    "        img = self.vis_processors[\"eval\"](raw_image).unsqueeze(0).to(self.device)\n",
    "        txt = self.text_processors[\"eval\"](caption)\n",
    "\n",
    "        # Compute GradCam\n",
    "        txt_tokens = self.model.tokenizer(txt, return_tensors=\"pt\").to(self.device)\n",
    "        gradcam, _ = compute_gradcam(self.model, img, txt, txt_tokens, block_num=block_num)\n",
    "\n",
    "        # Average GradCam for the full image\n",
    "        avg_gradcam = getAttMap(norm_img, gradcam[0][1], blur=True, overlap=False)\n",
    "        \n",
    "        if self.debug_vis_flag:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "            ax.imshow(avg_gradcam)\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xticks([])\n",
    "\n",
    "        overall_gradcam_wrt_tokens = []\n",
    "        \n",
    "        num_image = len(txt_tokens.input_ids[0]) - 2 + 1\n",
    "        if self.debug_vis_flag:\n",
    "            fig, ax = plt.subplots(num_image, 1, figsize=(15, 5 * num_image))\n",
    "\n",
    "        gradcam_iter = iter(gradcam[0][2:-1])\n",
    "        token_id_iter = iter(txt_tokens.input_ids[0][1:-1])\n",
    "\n",
    "        for i, (gradcam, token_id) in enumerate(zip(gradcam_iter, token_id_iter)):\n",
    "            word = self.model.tokenizer.decode([token_id])\n",
    "            gradcam_image = getAttMap(norm_img, gradcam, blur=True, overlap=False)\n",
    "            overall_gradcam_wrt_tokens.append(gradcam_image)\n",
    "            if self.debug_vis_flag:\n",
    "                ax[i].imshow(gradcam_image)\n",
    "                ax[i].set_yticks([])\n",
    "                ax[i].set_xticks([])\n",
    "                ax[i].set_xlabel(word)\n",
    "\n",
    "        overall_gradcam_wrt_tokens = np.stack(overall_gradcam_wrt_tokens, axis =0)\n",
    "        avg_gradcam_wrt_tokens = np.mean(overall_gradcam_wrt_tokens, axis=0)\n",
    "        \n",
    "        if self.debug_vis_flag:\n",
    "            ax[num_image-1].imshow(avg_gradcam_wrt_tokens)\n",
    "            ax[num_image-1].set_yticks([])\n",
    "            ax[num_image-1].set_xticks([])\n",
    "            ax[num_image-1].set_xlabel(caption)\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "        print(avg_gradcam.shape, avg_gradcam.dtype, np.max(avg_gradcam), np.min(avg_gradcam))\n",
    "        print(avg_gradcam_wrt_tokens.shape, avg_gradcam_wrt_tokens.dtype, np.max(avg_gradcam_wrt_tokens), np.min(avg_gradcam_wrt_tokens))\n",
    "        return avg_gradcam_wrt_tokens\n",
    "\n",
    "\n",
    "# model: pysaliency.SaliencyMapModel = LavisTextGuidedSaliencyModel(debug_vis_flag=True)\n",
    "# smap = model.saliency_map(mit_stimuli[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement you own saliency map model, inherit from `pysaliency.SaliencyMapModel` and implement the `_saliency_map` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------    Model: ours -----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-13 09:31:59.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlavis.models\u001b[0m:\u001b[36mload_model_and_preprocess\u001b[0m:\u001b[36m152\u001b[0m - \u001b[1mLoading the preprocessors from the default config file...\u001b[0m\n",
      "\u001b[32m2023-12-13 09:31:59.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlavis.models\u001b[0m:\u001b[36mload_model_and_preprocess\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1margs:{'model': {'arch': 'blip_image_text_matching', 'load_finetuned': True, 'finetuned': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_coco.pth', 'vit_type': 'large', 'vit_grad_ckpt': False, 'vit_ckpt_layer': 0, 'image_size': 384, 'med_config_path': 'configs/models/med_large_config.json', 'embed_dim': 256}, 'preprocess': {'vis_processor': {'eval': {'name': 'blip_image_eval', 'image_size': 384}}, 'text_processor': {'eval': {'name': 'blip_caption'}}}}\u001b[0m\n",
      "\u001b[32m2023-12-13 09:31:59.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlavis.models\u001b[0m:\u001b[36mload_preprocess\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mVisual pretrained model: {'eval': {'name': 'blip_image_eval', 'image_size': 384}} | Text pretrained model: {'eval': {'name': 'blip_caption'}}\u001b[0m\n",
      "Using cache found in /mnt/homes/minghao/.cache/torch/hub/pytorch_vision_v0.6.0\n",
      "/mnt/homes/minghao/anaconda3/envs/lavis/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/homes/minghao/anaconda3/envs/lavis/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~mit1003~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipykernel_1920060/1814523666.py:59: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  image_tensor = torch.tensor([image.transpose(2, 0, 1)]).to(DEVICE)\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.55s/it]\n",
      "100%|██████████| 10/10 [00:00<00:00, 340.17it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 413.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~mit1003_onesize~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.13it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 350.36it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 391.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~sjtuvis~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:11,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 640) float32 1.0 0.0\n",
      "(426, 640) float32 0.66914386 0.00788858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:01<00:04,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375, 500) float32 1.0 0.0\n",
      "(375, 500) float32 0.6562144 0.003291239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:03<00:00,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416, 640) float32 1.0 0.0\n",
      "(416, 640) float32 0.5651475 0.006198583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(536, 640) float32 1.0 0.0\n",
      "(536, 640) float32 0.6185588 0.00049086614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 2522.89it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 7835.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~figrim~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.48s/it]\n",
      "100%|██████████| 10/10 [00:00<00:00, 285.88it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 326.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~salicon_train~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.16it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 88.78it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 363.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~salicon_eval~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.31it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 81.61it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 351.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~toronto~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.98it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 728.92it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 851.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------    Model: deepgazeIIE -----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homes/minghao/anaconda3/envs/lavis/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/homes/minghao/anaconda3/envs/lavis/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/homes/minghao/.cache/torch/hub/pytorch_vision_v0.6.0\n",
      "/mnt/homes/minghao/anaconda3/envs/lavis/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in /mnt/homes/minghao/.cache/torch/hub/pytorch_vision_v0.6.0\n",
      "/mnt/homes/minghao/anaconda3/envs/lavis/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~mit1003~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:06<00:04,  1.11s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model_zoos = [  \"SUN\", \"IttiKoch\", \"Judd\", \"CovSal\",\"AIM\", \"ours\", \"deepgazeIIE\", \"deepgazeI\", \"deepgazeIII\"]\n",
    "import pysaliency\n",
    "from pysaliency.external_models import AIM, SUN, GBVSIttiKoch, Judd, IttiKoch, CovSal\n",
    "from pysaliency.external_datasets.sjtuvis import TextDescriptor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pysaliency.saliency_map_conversion import optimize_for_information_gain\n",
    "\n",
    "from pysaliency.external_datasets import get_mit1003, get_sjtu_vis, get_cat2000_train, get_FIGRIM, get_mit300, get_mit1003, get_mit1003_onesize, get_SALICON, get_toronto, get_DUT_OMRON, get_OSIE, get_PASCAL_S, get_NUSEF_public\n",
    "\n",
    "DATASET_MAPPINGS = {\n",
    "\t\"mit1003\": get_mit1003,\n",
    " \t\"mit1003_onesize\": get_mit1003_onesize,\n",
    " \t\"sjtuvis\": get_sjtu_vis,\n",
    "\t# \"cat2000_train\": get_cat2000_train,\n",
    "\t\"figrim\": get_FIGRIM,\n",
    "\t\"salicon_eval\": get_SALICON,\n",
    "\t\"toronto\": get_toronto,\n",
    " \t\"DUT_OMRON\": get_DUT_OMRON,\n",
    "\t\"OSIE\": get_OSIE,\n",
    "\t\"PASCAL_S\": get_PASCAL_S,\n",
    "\t\"NUSEF_public\": get_NUSEF_public\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "columns = ['Model', 'Dataset', 'AUC_shuffled', 'AUC_uniform', 'KL_uniform', 'KL_shuffled', 'KL_identical_nonfixations', 'Image_based_KL_divergence']\n",
    "results_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "for model_name in model_zoos[:5]:\n",
    "\tprint(f\"------------------------    Model: {model_name} -----------------------------------\")\n",
    "\tmodel: pysaliency.SaliencyMapModel\n",
    "\tmodel_location = \"../../models/\"\n",
    "\tif model_name == \"deepgazeI\":\n",
    "\t\tmodel: pysaliency.SaliencyMapModel = DeepGazeSaliencyModel(version=1)\n",
    "\telif model_name == \"deepgazeIIE\":\n",
    "\t\tmodel: pysaliency.SaliencyMapModel = DeepGazeSaliencyModel(version=2)\n",
    "\telif model_name == \"deepgazeIII\":\n",
    "\t\tmodel: pysaliency.SaliencyMapModel = DeepGazeSaliencyModel(version=3)\n",
    "\telif model_name == \"ours\":\n",
    "\t\tmodel: pysaliency.SaliencyMapModel = LavisTextGuidedSaliencyModel()\n",
    "\telif model_name == \"GoldStandard\":\n",
    "\t\traise NotImplementedError\n",
    "\telse:\n",
    "\t\tmodel: pysaliency.SaliencyMapModel = eval(model_name)(location = model_location, cache_location=os.path.join('../../datasets/model_caches/', model_name), caching=True)\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\tfor dataset_key, dataset_func in DATASET_MAPPINGS.items():\n",
    "\t\tdataset_location = \"../../datasets/\"\n",
    "\t\tprint(f\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~{dataset_key}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "\t\tif dataset_key == \"sjtuvis\":\n",
    "\t\t\ttext_descriptor = TextDescriptor('../../datasets/test/original_sjtuvis_dataset/text.xlsx')\n",
    "\t\t\tmit_stimuli, mit_fixations = dataset_func(original_dataset_path=\"../../datasets/test/original_sjtuvis_dataset\", location=dataset_location, text_descriptor=text_descriptor)\n",
    "\t\telif dataset_key == \"cat2000_train\":\n",
    "\t\t\tmit_stimuli, mit_fixations = dataset_func(location=dataset_location, version = \"1.1\")\n",
    "\t\telif dataset_key == \"salicon_train\":\n",
    "\t\t\tstimuli_train, stimuli_val, stimuli_test, fixations_train, fixations_val = dataset_func(location=dataset_location)\n",
    "\t\t\tmit_stimuli, mit_fixations = stimuli_train, fixations_train\n",
    "\t\telif dataset_key == \"salicon_eval\":\n",
    "\t\t\tstimuli_train, stimuli_val, stimuli_test, fixations_train, fixations_val = dataset_func(location=dataset_location)\n",
    "\t\t\tmit_stimuli, mit_fixations = stimuli_val, fixations_val\n",
    "\t\telse:\n",
    "\t\t\tmit_stimuli, mit_fixations = dataset_func(location=dataset_location)\n",
    "\t\t\t\n",
    "\t\tcutoff = 10\n",
    "\t\tshort_stimuli = pysaliency.FileStimuli(filenames=mit_stimuli.filenames[:cutoff])\n",
    "\t\tshort_fixations = mit_fixations[mit_fixations.n < cutoff]\n",
    "\t\t\n",
    "\t\t# try:\n",
    "\t\tauc_uniform = model.AUC(short_stimuli, short_fixations, nonfixations='uniform', verbose=True)\n",
    "\t\tauc_shuffled = model.AUC(short_stimuli, short_fixations, nonfixations='shuffled', verbose=True)\n",
    "\t\tauc_identical_nonfixations = model.AUC(short_stimuli, short_fixations, nonfixations=short_fixations, verbose=True)\n",
    "\t\tkl_uniform = model.fixation_based_KL_divergence(short_stimuli, short_fixations, nonfixations='uniform')\n",
    "\t\t# kl_shuffled = model.fixation_based_KL_divergence(short_stimuli, short_fixations, nonfixations='shuffled')\n",
    "\t\tkl_identical = model.fixation_based_KL_divergence(short_stimuli, short_fixations, nonfixations=short_fixations)\n",
    "\t\tnss = model.NSS(short_stimuli, short_fixations)\n",
    "\t\tgold_standard = pysaliency.FixationMap(short_stimuli, short_fixations, kernel_size=30)\n",
    "\t\timage_based_kl = model.image_based_kl_divergence(short_stimuli, gold_standard)\n",
    "\t\tcc = model.CC(short_stimuli, gold_standard)\n",
    "\t\tssim = model.SIM(short_stimuli, gold_standard)\n",
    "\t\t# ig = optimize_for_information_gain(model, short_stimuli, short_fixations)\n",
    "\t\t# except Exception as e:\n",
    "      \n",
    "\t\t# \tauc_uniform, auc_shuffled, auc_identical_nonfixations, kl_uniform, kl_identical, nss, image_based_kl, cc, ssim = np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "\t\tresult = {\n",
    "\t\t\t'Model': model_name,\n",
    "\t\t\t'Dataset': dataset_key,\n",
    "\t\t\t'AUC_shuffled': auc_shuffled,\n",
    "\t\t\t'AUC_uniform': auc_uniform,\n",
    "\t\t\t'NSS': nss,\n",
    "\t\t\t'KL_uniform': kl_uniform,\t\n",
    "\t\t\t# 'KL_shuffled': kl_shuffled,\n",
    "\t\t\t'KL_identical_nonfixations': kl_identical,\n",
    "\t\t\t'Image_based_KL_divergence': image_based_kl,\n",
    "\t\t\t\"cc\": cc,\n",
    "   \t\t\t\"ssim\": ssim\n",
    "\t\t}\n",
    "\t\tresults_df = pd.concat([results_df, pd.DataFrame([result])], ignore_index=True)\n",
    "\t\t\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/liilu/Desktop/COURSE/ImageProcessing/Assignment2/BASELINE/pysaliency/notebooks/visualization.ipynb Cell 21\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/liilu/Desktop/COURSE/ImageProcessing/Assignment2/BASELINE/pysaliency/notebooks/visualization.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Display the DataFrame\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/liilu/Desktop/COURSE/ImageProcessing/Assignment2/BASELINE/pysaliency/notebooks/visualization.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# results_df.to_csv(\"results.csv\")\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/liilu/Desktop/COURSE/ImageProcessing/Assignment2/BASELINE/pysaliency/notebooks/visualization.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(results_df)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/liilu/Desktop/COURSE/ImageProcessing/Assignment2/BASELINE/pysaliency/notebooks/visualization.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/liilu/Desktop/COURSE/ImageProcessing/Assignment2/BASELINE/pysaliency/notebooks/visualization.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Display the DataFrame\n",
    "# results_df.to_csv(\"results.csv\")\n",
    "print(results_df)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='AUC_shuffled', hue='Dataset')\n",
    "plt.title('AUC Shuffled by Model and Dataset')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lavis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "signature": "sha256:917c6c1be26dfc1ef9f339ee9292cd5d6cdd37d31b85227622e255bbd6a36b07"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
