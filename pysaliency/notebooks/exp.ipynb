{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pysaliency\n",
    "==========\n",
    "\n",
    "Saliency Map Models\n",
    "----------------------\n",
    "\n",
    "`pysaliency` comes with a variety of features to evaluate saliency map models. This notebooks demonstrates these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the MIT1003 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3402960091.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    data_location = \"../../datasets/test\"|\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pysaliency\n",
    "from pysaliency.external_datasets.sjtuvis import TextDescriptor\n",
    "\n",
    "data_location = \"../../datasets/test\"|\n",
    "text_descriptor = TextDescriptor('../../datasets/test/original_sjtuvis_dataset/text.xlsx')\n",
    "print(text_descriptor.get_description('000000020777_2.png')) \n",
    "mit_stimuli, mit_fixations = pysaliency.external_datasets.get_sjtu_vis(\"../../datasets/test/original_sjtuvis_dataset\", location=data_location, text_descriptor=text_descriptor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As some evaluation methods can take quite a long time to run, we prepare a smaller dataset consisting of only the first 10 stimuli:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_location = \"../../datasets\"\n",
    "mit_stimuli, mit_fixations = pysaliency.external_datasets.get_mit1003(location=data_location)\n",
    "index = 10\n",
    "plt.imshow(mit_stimuli.stimuli[index])\n",
    "f = mit_fixations[mit_fixations.n == index]\n",
    "plt.scatter(f.x, f.y, color='r')\n",
    "_ = plt.axis('off')\n",
    "cutoff = 20\n",
    "\n",
    "aim: pysaliency.SaliencyMapModel = pysaliency.AIM(location='../../models', cache_location=os.path.join('model_caches', 'AIM'))\n",
    "\n",
    "from pysaliency.external_datasets.sjtuvis import TextDescriptor\n",
    "text_descriptor = TextDescriptor('../../datasets/test/original_sjtuvis_dataset/text.xlsx')\n",
    "data_location = \"../../datasets/test\"\n",
    "original_dataset_path = \"../../datasets/test/original_sjtuvis_dataset\"\n",
    "mit_stimuli, mit_fixations = pysaliency.external_datasets.get_sjtu_vis(original_dataset_path=original_dataset_path, location=data_location, text_descriptor = text_descriptor)\n",
    "short_stimuli = pysaliency.FileStimuli(filenames=mit_stimuli.filenames[:cutoff])\n",
    "short_fixations = mit_fixations[mit_fixations.n < cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lavis text guided saliency model. Loaded from the lavis module. ðŸ˜Ž ðŸ˜Ž ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from lavis.models import load_model_and_preprocess\n",
    "from lavis.processors import load_processor\n",
    "from matplotlib import pyplot as plt\n",
    "from lavis.common.gradcam import getAttMap\n",
    "from lavis.models.blip_models.blip_image_text_matching import compute_gradcam\n",
    "import numpy as np\n",
    "from pysaliency.datasets import Stimulus, Fixations,  StimuliStimulus\n",
    "import pysaliency\n",
    "\n",
    "def handle_stimulus(stimulus):\n",
    "    \"\"\"\n",
    "    Make sure that a stimulus is a `Stimulus`-object\n",
    "    \"\"\"\n",
    "    if not isinstance(stimulus, Stimulus):\n",
    "        stimulus = Stimulus(stimulus)\n",
    "    return stimulus\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.misc import face\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.special import logsumexp\n",
    "import torch\n",
    "\n",
    "import deepgaze_pytorch    \n",
    "from typing import Optional, Tuple, List, Dict, Any, Union, Sequence\n",
    "import pysaliency\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "class DeepGazeSaliencyModel(pysaliency.SaliencyMapModel):\n",
    "    \n",
    "    def __init__(self, block_num=7, dst_w=720, version = 1):\n",
    "        super().__init__()\n",
    "        if version == 1:\n",
    "            self.model = deepgaze_pytorch.DeepGazeI(pretrained=True).to(DEVICE)\n",
    "            self.use_hist_scan_path = False\n",
    "        elif version == 2:\n",
    "            self.model = deepgaze_pytorch.DeepGazeIIE(pretrained=True).to(DEVICE)\n",
    "            self.use_hist_scan_path = False\n",
    "        elif version == 3:\n",
    "            self.model = deepgaze_pytorch.DeepGazeIII(pretrained=True).to(DEVICE)\n",
    "            self.use_hist_scan_path = True\n",
    "        self.block_num = block_num\n",
    "        self.dst_w = dst_w\n",
    "        \n",
    "    def _saliency_map(self, stimulus, text_description:Optional[str]=None):\n",
    "        image = stimulus\n",
    "        # print(\"The input image....\")\n",
    "        # print(image.shape, image.dtype, np.min(image), np.max(image))\n",
    "        fixation_history_x = np.array([1024//2, 300, 500, 200, 200, 700])\n",
    "        fixation_history_y = np.array([768//2, 300, 100, 300, 100, 500])\n",
    "\n",
    "        centerbias_template = np.load('../../pretrained_weights/deepgaze/centerbias_mit1003.npy')\n",
    "        centerbias = zoom(centerbias_template, (image.shape[0]/centerbias_template.shape[0], image.shape[1]/centerbias_template.shape[1]), order=0, mode='nearest')\n",
    "        centerbias -= logsumexp(centerbias)\n",
    "\n",
    "        image_tensor = torch.tensor([image.transpose(2, 0, 1)]).to(DEVICE)\n",
    "        centerbias_tensor = torch.tensor([centerbias]).to(DEVICE)\n",
    "\n",
    "        if self.use_hist_scan_path:\n",
    "            x_hist_tensor = torch.tensor([fixation_history_x[self.model.included_fixations]]).to(DEVICE)\n",
    "            y_hist_tensor = torch.tensor([fixation_history_y[self.model.included_fixations]]).to(DEVICE)\n",
    "            log_density_prediction = self.model(image_tensor, centerbias_tensor, x_hist_tensor, y_hist_tensor)\n",
    "        else:\n",
    "            log_density_prediction = self.model(image_tensor, centerbias_tensor)\n",
    "\n",
    "        # f, axs = plt.subplots(nrows=1, ncols=2, figsize=(8, 3))\n",
    "        # axs[0].imshow(image)\n",
    "        # axs[0].plot(fixation_history_x, fixation_history_y, 'o-', color='red')\n",
    "        # axs[0].scatter(fixation_history_x[-1], fixation_history_y[-1], 100, color='yellow', zorder=100)\n",
    "        # axs[0].set_axis_off()\n",
    "        # axs[1].matshow(log_density_prediction.detach().cpu().numpy()[0, 0])  # first image in batch, first (and only) channel\n",
    "        # axs[1].plot(fixation_history_x, fixation_history_y, 'o-', color='red')\n",
    "        # axs[1].scatter(fixation_history_x[-1], fixation_history_y[-1], 100, color='yellow', zorder=100)\n",
    "        # axs[1].set_axis_off()\n",
    "        if log_density_prediction.dim() == 2:\n",
    "            pass\n",
    "        elif log_density_prediction.dim() == 4:\n",
    "            log_density_prediction = log_density_prediction[0, 0]\n",
    "        elif log_density_prediction.dim() == 3:\n",
    "            print(log_density_prediction.shape)\n",
    "            assert log_density_prediction.shape[0] == 1\n",
    "            log_density_prediction = log_density_prediction[0]\n",
    "        assert log_density_prediction.dim() == 2, \"log_density_prediction should be a 3D tensor, but is {}\".format(log_density_prediction.shape)\n",
    "        return log_density_prediction.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "class LavisTextGuidedSaliencyModel(pysaliency.SaliencyMapModel):\n",
    "    \n",
    "    def __init__(self, debug_vis_flag = False, cache_location=None, caching=True, memory_cache_size=None):\n",
    "        super().__init__(cache_location, caching, memory_cache_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model, self.vis_processors, self.text_processors = load_model_and_preprocess(\"blip_image_text_matching\", \"large\", device=self.device, is_eval=True)\n",
    "        self.debug_vis_flag = debug_vis_flag\n",
    "        self.deepgaze_model = DeepGazeSaliencyModel(version=3)\n",
    "        \n",
    "    def saliency_map(self, stimulus):\n",
    "        \"\"\"\n",
    "        Get saliency map for given stimulus.\n",
    "\n",
    "        To overwrite this function, overwrite `_saliency_map` as otherwise\n",
    "        the caching mechanism is disabled.\n",
    "        \"\"\"\n",
    "        if isinstance(stimulus, StimuliStimulus):\n",
    "            filename = os.path.basename(stimulus.filename)\n",
    "        elif isinstance(stimulus, Stimulus):\n",
    "            filename = None\n",
    "        elif isinstance(stimulus, np.ndarray):\n",
    "            filename = None\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        stimulus = handle_stimulus(stimulus)\n",
    "            \n",
    "        try:\n",
    "            text_description=text_descriptor.get_description(filename)\n",
    "        except Exception as e:\n",
    "            text_description=None\n",
    "        # print(text_description, stimulus.filename)\n",
    "        if text_description is None:\n",
    "            return self.deepgaze_model.saliency_map(stimulus.stimulus_data)\n",
    "        else:\n",
    "            # print(text_description)\n",
    "            pass\n",
    "        if not self.caching:\n",
    "            return self._saliency_map(stimulus.stimulus_data, text_description=text_description)\n",
    "        stimulus_id = stimulus.stimulus_id\n",
    "        if not stimulus_id in self._cache:\n",
    "            temp = stimulus.stimulus_data\n",
    "            self._cache[stimulus_id] = self._saliency_map(temp, text_description=text_description)\n",
    "        return self._cache[stimulus_id]\n",
    "    \n",
    "    def _saliency_map(self, stimulus, text_description=None):\n",
    "        \n",
    "        # print(f\"****************      {text_description}      *********************\")\n",
    "        \n",
    "        return self.evaluate_saliency_map(stimulus[...,:3], \"large\", text_description)\n",
    "    \n",
    "    def evaluate_saliency_map(self, raw_image, model_path, caption, block_num=7, dst_w=720):\n",
    "        # print(raw_image.shape, raw_image.dtype, np.max(raw_image), np.min(raw_image))\n",
    "        raw_image = Image.fromarray(np.uint8(raw_image)).convert('RGB')\n",
    "        norm_img = np.float32(raw_image) / 255\n",
    "\n",
    "        # Preprocess image and text inputs\n",
    "        img = self.vis_processors[\"eval\"](raw_image).unsqueeze(0).to(self.device)\n",
    "        txt = self.text_processors[\"eval\"](caption)\n",
    "\n",
    "        # Compute GradCam\n",
    "        txt_tokens = self.model.tokenizer(txt, return_tensors=\"pt\").to(self.device)\n",
    "        gradcam, _ = compute_gradcam(self.model, img, txt, txt_tokens, block_num=block_num)\n",
    "\n",
    "        # Average GradCam for the full image\n",
    "        avg_gradcam = getAttMap(norm_img, gradcam[0][1], blur=True, overlap=False)\n",
    "        \n",
    "        if self.debug_vis_flag:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "            ax.imshow(avg_gradcam)\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xticks([])\n",
    "\n",
    "        overall_gradcam_wrt_tokens = []\n",
    "        \n",
    "        num_image = len(txt_tokens.input_ids[0]) - 2 + 1\n",
    "        if self.debug_vis_flag:\n",
    "            fig, ax = plt.subplots(num_image, 1, figsize=(15, 5 * num_image))\n",
    "\n",
    "        gradcam_iter = iter(gradcam[0][2:-1])\n",
    "        token_id_iter = iter(txt_tokens.input_ids[0][1:-1])\n",
    "\n",
    "        for i, (gradcam, token_id) in enumerate(zip(gradcam_iter, token_id_iter)):\n",
    "            word = self.model.tokenizer.decode([token_id])\n",
    "            gradcam_image = getAttMap(norm_img, gradcam, blur=True, overlap=False)\n",
    "            overall_gradcam_wrt_tokens.append(gradcam_image)\n",
    "            if self.debug_vis_flag:\n",
    "                ax[i].imshow(gradcam_image)\n",
    "                ax[i].set_yticks([])\n",
    "                ax[i].set_xticks([])\n",
    "                ax[i].set_xlabel(word)\n",
    "\n",
    "        overall_gradcam_wrt_tokens = np.stack(overall_gradcam_wrt_tokens, axis =0)\n",
    "        avg_gradcam_wrt_tokens = np.mean(overall_gradcam_wrt_tokens, axis=0)\n",
    "        \n",
    "        if self.debug_vis_flag:\n",
    "            ax[num_image-1].imshow(avg_gradcam_wrt_tokens)\n",
    "            ax[num_image-1].set_yticks([])\n",
    "            ax[num_image-1].set_xticks([])\n",
    "            ax[num_image-1].set_xlabel(caption)\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "        print(avg_gradcam.shape, avg_gradcam.dtype, np.max(avg_gradcam), np.min(avg_gradcam))\n",
    "        print(avg_gradcam_wrt_tokens.shape, avg_gradcam_wrt_tokens.dtype, np.max(avg_gradcam_wrt_tokens), np.min(avg_gradcam_wrt_tokens))\n",
    "        return avg_gradcam_wrt_tokens\n",
    "\n",
    "\n",
    "# model: pysaliency.SaliencyMapModel = LavisTextGuidedSaliencyModel(debug_vis_flag=True)\n",
    "# smap = model.saliency_map(mit_stimuli[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_zoos = [  \"SUN\", \"IttiKoch\", \"Judd\", \"CovSal\",\"AIM\", \"ours\", \"deepgazeIIE\", \"deepgazeI\", \"deepgazeIII\"]\n",
    "import pysaliency\n",
    "from pysaliency.external_models import AIM, SUN, GBVSIttiKoch, Judd, IttiKoch, CovSal\n",
    "from pysaliency.external_datasets.sjtuvis import TextDescriptor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pysaliency.saliency_map_conversion import optimize_for_information_gain\n",
    "\n",
    "from pysaliency.external_datasets import get_mit1003, get_sjtu_vis, get_cat2000_train, get_FIGRIM, get_mit300, get_mit1003, get_mit1003_onesize, get_SALICON, get_toronto, get_DUT_OMRON, get_OSIE, get_PASCAL_S, get_NUSEF_public\n",
    "\n",
    "DATASET_MAPPINGS = {\n",
    "\t\"mit1003\": get_mit1003,\n",
    " \t\"mit1003_onesize\": get_mit1003_onesize,\n",
    " \t\"sjtuvis\": get_sjtu_vis,\n",
    "\t# \"cat2000_train\": get_cat2000_train,\n",
    "\t\"figrim\": get_FIGRIM,\n",
    "\t\"salicon_eval\": get_SALICON,\n",
    "\t\"toronto\": get_toronto,\n",
    " \t\"DUT_OMRON\": get_DUT_OMRON,\n",
    "\t\"OSIE\": get_OSIE,\n",
    "\t\"PASCAL_S\": get_PASCAL_S,\n",
    "\t\"NUSEF_public\": get_NUSEF_public\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "columns = ['Model', 'Dataset', 'AUC_shuffled', 'AUC_uniform', 'KL_uniform', 'KL_shuffled', 'KL_identical_nonfixations', 'Image_based_KL_divergence']\n",
    "results_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "for model_name in model_zoos[5:]:\n",
    "\tprint(f\"------------------------    Model: {model_name} -----------------------------------\")\n",
    "\tmodel: pysaliency.SaliencyMapModel\n",
    "\tmodel_location = \"../../models/\"\n",
    "\tif model_name == \"deepgazeI\":\n",
    "\t\tmodel: pysaliency.SaliencyMapModel = DeepGazeSaliencyModel(version=1)\n",
    "\telif model_name == \"deepgazeIIE\":\n",
    "\t\tmodel: pysaliency.SaliencyMapModel = DeepGazeSaliencyModel(version=2)\n",
    "\telif model_name == \"deepgazeIII\":\n",
    "\t\tmodel: pysaliency.SaliencyMapModel = DeepGazeSaliencyModel(version=3)\n",
    "\telif model_name == \"ours\":\n",
    "\t\tmodel: pysaliency.SaliencyMapModel = LavisTextGuidedSaliencyModel()\n",
    "\telif model_name == \"GoldStandard\":\n",
    "\t\traise NotImplementedError\n",
    "\telse:\n",
    "\t\tmodel: pysaliency.SaliencyMapModel = eval(model_name)(location = model_location, cache_location=os.path.join('../../datasets/model_caches/', model_name), caching=True)\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\tfor dataset_key, dataset_func in DATASET_MAPPINGS.items():\n",
    "\t\tdataset_location = \"../../datasets/\"\n",
    "\t\tprint(f\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~{dataset_key}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "\t\tif dataset_key == \"sjtuvis\":\n",
    "\t\t\ttext_descriptor = TextDescriptor('../../datasets/test/original_sjtuvis_dataset/text.xlsx')\n",
    "\t\t\tmit_stimuli, mit_fixations = dataset_func(original_dataset_path=\"../../datasets/test/original_sjtuvis_dataset\", location=dataset_location, text_descriptor=text_descriptor)\n",
    "\t\telif dataset_key == \"cat2000_train\":\n",
    "\t\t\tmit_stimuli, mit_fixations = dataset_func(location=dataset_location, version = \"1.1\")\n",
    "\t\telif dataset_key == \"salicon_train\":\n",
    "\t\t\tstimuli_train, stimuli_val, stimuli_test, fixations_train, fixations_val = dataset_func(location=dataset_location)\n",
    "\t\t\tmit_stimuli, mit_fixations = stimuli_train, fixations_train\n",
    "\t\telif dataset_key == \"salicon_eval\":\n",
    "\t\t\tstimuli_train, stimuli_val, stimuli_test, fixations_train, fixations_val = dataset_func(location=dataset_location)\n",
    "\t\t\tmit_stimuli, mit_fixations = stimuli_val, fixations_val\n",
    "\t\telse:\n",
    "\t\t\tmit_stimuli, mit_fixations = dataset_func(location=dataset_location)\n",
    "\t\t\t\n",
    "\t\tcutoff = 10\n",
    "\t\tshort_stimuli = pysaliency.FileStimuli(filenames=mit_stimuli.filenames[:cutoff])\n",
    "\t\tshort_fixations = mit_fixations[mit_fixations.n < cutoff]\n",
    "\t\t\n",
    "\t\ttry:\n",
    "\t\t\tauc_uniform = model.AUC(short_stimuli, short_fixations, nonfixations='uniform', verbose=True)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tauc_uniform = np.nan\n",
    "\t\ttry:\n",
    "\t\t\tauc_shuffled = model.AUC(short_stimuli, short_fixations, nonfixations='shuffled', verbose=True)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tauc_shuffled = np.nan\n",
    "\t\ttry:\n",
    "\t\t\tauc_identical_nonfixations = model.AUC(short_stimuli, short_fixations, nonfixations=short_fixations, verbose=True)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tauc_identical_nonfixations = np.nan\n",
    "\t\ttry:\n",
    "\t\t\tkl_uniform = model.fixation_based_KL_divergence(short_stimuli, short_fixations, nonfixations='uniform')\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tkl_uniform = np.nan\n",
    "\t\t\t\n",
    "\t\ttry:\n",
    "\t\t\tkl_identical = model.fixation_based_KL_divergence(short_stimuli, short_fixations, nonfixations=short_fixations)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tkl_identical = np.nan\n",
    "\t\ttry:\n",
    "\t\t\tnss = model.NSS(short_stimuli, short_fixations)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tnss = np.nan\n",
    "\t\ttry:\n",
    "\t\t\tgold_standard = pysaliency.FixationMap(short_stimuli, short_fixations, kernel_size=30)\n",
    "\t\t\timage_based_kl = model.image_based_kl_divergence(short_stimuli, gold_standard)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\timage_based_kl = np.nan\t\n",
    "\t\ttry:\n",
    "\t\t\tcc = model.CC(short_stimuli, gold_standard)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tcc = np.nan\n",
    "\t\ttry:\n",
    "\t\t\tssim = model.SIM(short_stimuli, gold_standard)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tssim = np.nan\n",
    "\n",
    "\t\tresult = {\n",
    "\t\t\t'Model': model_name,\n",
    "\t\t\t'Dataset': dataset_key,\n",
    "\t\t\t'AUC_shuffled': auc_shuffled,\n",
    "\t\t\t'AUC_uniform': auc_uniform,\n",
    "\t\t\t'NSS': nss,\n",
    "\t\t\t'KL_uniform': kl_uniform,\t\n",
    "\t\t\t'KL_identical_nonfixations': kl_identical,\n",
    "\t\t\t'Image_based_KL_divergence': image_based_kl,\n",
    "\t\t\t\"cc\": cc,\n",
    "   \t\t\t\"ssim\": ssim\n",
    "\t\t}\n",
    "\t\tresults_df = pd.concat([results_df, pd.DataFrame([result])], ignore_index=True)\n",
    "\t\t\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement you own saliency map model, inherit from `pysaliency.SaliencyMapModel` and implement the `_saliency_map` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the DataFrame\n",
    "# results_df.to_csv(\"results.csv\")\n",
    "print(results_df)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='AUC_shuffled', hue='Dataset')\n",
    "plt.title('AUC Shuffled by Model and Dataset')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='AUC_uniform', hue='Dataset')\n",
    "plt.title('AUC Shuffled by Model and Dataset')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='NSS', hue='Dataset')\n",
    "plt.title('AUC Shuffled by Model and Dataset')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='KL_uniform', hue='Dataset')\n",
    "plt.title('AUC Shuffled by Model and Dataset')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='KL_identical_nonfixations', hue='Dataset')\n",
    "plt.title('AUC Shuffled by Model and Dataset')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='Image_based_KL_divergence', hue='Dataset')\n",
    "plt.title('AUC Shuffled by Model and Dataset')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='cc', hue='Dataset')\n",
    "plt.title('AUC Shuffled by Model and Dataset')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='ssim', hue='Dataset')\n",
    "plt.title('AUC Shuffled by Model and Dataset')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lavis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "signature": "sha256:917c6c1be26dfc1ef9f339ee9292cd5d6cdd37d31b85227622e255bbd6a36b07"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
