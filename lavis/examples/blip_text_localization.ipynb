{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Preprocess image and text inputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2023-12-14 14:25:09.869036: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64:/usr/lib/nvidia\n",
                        "2023-12-14 14:25:09.869052: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Two people looking at a large kitchen  000000000338_0.png\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32m2023-12-14 08:51:45.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlavis.models\u001b[0m:\u001b[36mload_model_and_preprocess\u001b[0m:\u001b[36m152\u001b[0m - \u001b[1mLoading the preprocessors from the default config file...\u001b[0m\n",
                        "\u001b[32m2023-12-14 08:51:45.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlavis.models\u001b[0m:\u001b[36mload_model_and_preprocess\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1margs:{'model': {'arch': 'blip_image_text_matching', 'load_finetuned': True, 'finetuned': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_coco.pth', 'vit_type': 'large', 'vit_grad_ckpt': False, 'vit_ckpt_layer': 0, 'image_size': 384, 'med_config_path': 'configs/models/med_large_config.json', 'embed_dim': 256}, 'preprocess': {'vis_processor': {'eval': {'name': 'blip_image_eval', 'image_size': 384}}, 'text_processor': {'eval': {'name': 'blip_caption'}}}}\u001b[0m\n",
                        "\u001b[32m2023-12-14 08:51:45.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlavis.models\u001b[0m:\u001b[36mload_preprocess\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mVisual pretrained model: {'eval': {'name': 'blip_image_eval', 'image_size': 384}} | Text pretrained model: {'eval': {'name': 'blip_caption'}}\u001b[0m\n"
                    ]
                },
                {
                    "ename": "OutOfMemoryError",
                    "evalue": "CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacty of 5.80 GiB of which 1.44 MiB is free. Including non-PyTorch memory, this process has 5.79 GiB memory in use. Of the allocated memory 4.45 GiB is allocated by PyTorch, and 139.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
                        "\u001b[1;32m/home/liilu/Desktop/COURSE/ImageProcessing/Assignment2/BASELINE/lavis/examples/blip_text_localization.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/liilu/Desktop/COURSE/ImageProcessing/Assignment2/BASELINE/lavis/examples/blip_text_localization.ipynb#X33sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/liilu/Desktop/COURSE/ImageProcessing/Assignment2/BASELINE/lavis/examples/blip_text_localization.ipynb#X33sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m caption \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMerlion near marina bay.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/liilu/Desktop/COURSE/ImageProcessing/Assignment2/BASELINE/lavis/examples/blip_text_localization.ipynb#X33sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m model, vis_processors, text_processors \u001b[39m=\u001b[39m load_model_and_preprocess(\u001b[39m\"\u001b[39;49m\u001b[39mblip_image_text_matching\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mlarge\u001b[39;49m\u001b[39m\"\u001b[39;49m, device\u001b[39m=\u001b[39;49mdevice, is_eval\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/liilu/Desktop/COURSE/ImageProcessing/Assignment2/BASELINE/lavis/examples/blip_text_localization.ipynb#X33sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m dst_w \u001b[39m=\u001b[39m \u001b[39m720\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/liilu/Desktop/COURSE/ImageProcessing/Assignment2/BASELINE/lavis/examples/blip_text_localization.ipynb#X33sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m raw_image \u001b[39m=\u001b[39m stimulus\n",
                        "File \u001b[0;32m~/Desktop/COURSE/ImageProcessing/Assignment2/BASELINE/lavis/lavis/models/__init__.py:167\u001b[0m, in \u001b[0;36mload_model_and_preprocess\u001b[0;34m(name, model_type, is_eval, device)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m device \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    165\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m--> 167\u001b[0m \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mto(device), vis_processors, txt_processors\n",
                        "File \u001b[0;32m~/.conda/envs/lavis/lib/python3.8/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
                        "File \u001b[0;32m~/.conda/envs/lavis/lib/python3.8/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
                        "File \u001b[0;32m~/.conda/envs/lavis/lib/python3.8/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
                        "File \u001b[0;32m~/.conda/envs/lavis/lib/python3.8/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
                        "File \u001b[0;32m~/.conda/envs/lavis/lib/python3.8/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
                        "File \u001b[0;32m~/.conda/envs/lavis/lib/python3.8/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
                        "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacty of 5.80 GiB of which 1.44 MiB is free. Including non-PyTorch memory, this process has 5.79 GiB memory in use. Of the allocated memory 4.45 GiB is allocated by PyTorch, and 139.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
                    ]
                },
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "from matplotlib import pyplot as plt\n",
                "from lavis.common.gradcam import getAttMap\n",
                "from lavis.models.blip_models.blip_image_text_matching import compute_gradcam\n",
                "import numpy as np\n",
                "from pysaliency.external_datasets.sjtuvis import TextDescriptor\n",
                "import pysaliency\n",
                "\n",
                "\n",
                "import torch\n",
                "from PIL import Image\n",
                "\n",
                "import sys, os\n",
                "# sys.path.append(\"../..\")\n",
                "from lavis.models import load_model_and_preprocess\n",
                "from lavis.processors import load_processor\n",
                "\n",
                "data_location = \"../../datasets/test\"\n",
                "text_descriptor = TextDescriptor('../../datasets/test/original_sjtuvis_dataset/text.xlsx')\n",
                "mit_stimuli, mit_fixations = pysaliency.external_datasets.get_sjtu_vis(\"../../datasets/test/original_sjtuvis_dataset\", location=data_location, text_descriptor=text_descriptor)\n",
                "\n",
                "idx = 15\n",
                "stimulus = Image.fromarray(mit_stimuli[idx].stimulus_data)\n",
                "filename = os.path.basename(mit_stimuli[idx].filename)\n",
                "text_description=text_descriptor.get_description(filename)\n",
                "print(text_description, filename)\n",
                " \n",
                "# setup device to use\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "model, vis_processors, text_processors = load_model_and_preprocess(\"blip_image_text_matching\", \"large\", device=device, is_eval=True)\n",
                "\n",
                "dst_w = 720\n",
                "raw_image = stimulus\n",
                "w, h = raw_image.size\n",
                "scaling_factor = dst_w / w\n",
                "\n",
                "resized_img = raw_image.resize((int(w * scaling_factor), int(h * scaling_factor)))\n",
                "norm_img = np.float32(resized_img) / 255\n",
                "\n",
                "img = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
                "txt = text_processors[\"eval\"](text_description)\n",
                "txt_tokens = model.tokenizer(txt, return_tensors=\"pt\").to(device)\n",
                "gradcam, _ = compute_gradcam(model, img, txt, txt_tokens, block_num=7)\n",
                "avg_gradcam = getAttMap(norm_img, gradcam[0][1], blur=True)\n",
                "# fig, ax = plt.subplots(num_image, 1, figsize=(15,5*num_image))\n",
                "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
                "ax.imshow(avg_gradcam)\n",
                "\n",
                "num_image = len(txt_tokens.input_ids[0]) - 2\n",
                "fig, ax = plt.subplots(num_image, 1, figsize=(15, 5 * num_image))\n",
                "\n",
                "gradcam_iter = iter(gradcam[0][2:-1])\n",
                "token_id_iter = iter(txt_tokens.input_ids[0][1:-1])\n",
                "\n",
                "for i, (gradcam, token_id) in enumerate(zip(gradcam_iter, token_id_iter)):\n",
                "    word = model.tokenizer.decode([token_id])\n",
                "    gradcam_image = getAttMap(norm_img, gradcam, blur=True)\n",
                "    ax[i].imshow(gradcam_image)\n",
                "    ax[i].set_yticks([])\n",
                "    ax[i].set_xticks([])\n",
                "    ax[i].set_xlabel(word)\n",
                "    \n",
                "    \n",
                "    # Maximum number of rows and columns per row\n",
                "max_rows = 10\n",
                "max_cols_per_row = 5  # Adjust based on desired layout\n",
                "\n",
                "# Determine the number of tokens to be displayed in each row\n",
                "num_tokens = len(txt_tokens.input_ids[0]) - 2\n",
                "tokens_per_row = min(max_cols_per_row - 2, num_tokens)  # Subtract 2 for original and avg_gradcam\n",
                "\n",
                "gradcam_iter = iter(gradcam[0][2:-1])\n",
                "token_id_iter = iter(txt_tokens.input_ids[0][1:-1])\n",
                "\n",
                "for row in range(max_rows):\n",
                "    # Create a subplot for this row\n",
                "    fig, axes = plt.subplots(1, tokens_per_row + 2, figsize=(15, 5))  # Adjust figsize as needed\n",
                "    \n",
                "    # Display the original image and avg_gradcam\n",
                "    axes[0].imshow(norm_img)\n",
                "    axes[0].set_title(\"Original Image\")\n",
                "    axes[1].imshow(avg_gradcam)\n",
                "    axes[1].set_title(\"Avg GradCAM\")\n",
                "\n",
                "    for col in range(2, tokens_per_row + 2):\n",
                "        try:\n",
                "            gradcam, token_id = next(zip(gradcam_iter, token_id_iter))\n",
                "        except StopIteration:\n",
                "            break  # No more tokens to display\n",
                "\n",
                "        word = model.tokenizer.decode([token_id])\n",
                "        gradcam_image = getAttMap(norm_img, gradcam, blur=True)\n",
                "        axes[col].imshow(gradcam_image)\n",
                "        axes[col].set_title(word)\n",
                "\n",
                "    for ax in axes:\n",
                "        ax.set_yticks([])\n",
                "        ax.set_xticks([])\n",
                "\n",
                "    plt.subplots_adjust(wspace=0, hspace=0)  # Remove gaps between subplots\n",
                "    plt.show()\n",
                "\n",
                "    if row * tokens_per_row >= num_tokens:\n",
                "        break  # Stop if we have displayed all tokens"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.18"
        },
        "vscode": {
            "interpreter": {
                "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
